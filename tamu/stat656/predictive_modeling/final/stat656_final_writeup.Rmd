---
output:
  pdf_document: default
  html_document: default
---
--
title: "Final Part 1, Predictions"
output:
  pdf_document
documentclass: article
classoption: a4paper
---

The analysis of the given data was divided into the following steps:

1 Examining the data structure and transforming the data
2 Evaluating and handling missing data
3 Evaluating the presence of multicollinearity among the features
4 Modeling the data


Step 1: Examining the data structure and transforming the data

- Dataset structure
  - The Xtrain dataset contained 5000 observations and 255 features, 2 of which were factors, the rest numeric.
  - The Xtrain dataset had 6287 missing values, but no more than 12 in any given feature, and the distribution appeared to be Missing at Random.
  - The Xvalidate dataset contained 1000 observations and 255 features, 2 of which were factors, the rest numeric.
  - The Xvalidate dataset had 1251 missing values, but no more than 5 in any given feature, and the distribution appeared to be Missing at Random.
  - The Xtest dataset contained 1000 observations and 255 features, 2 of which were factors, the rest numeric.
  - The Xtest dataset had 1274 missing values, but no more than 4 in any given feature, and the distribution appeared to be Missing at Random.
  - The Ytrain dataset contained 5000 values with 2 factors, 'fraud' and 'not fraud'
  - The Yvalidate dataset contained 1000 values with 2 factors, 'fraud' and 'not fraud'
- Data transformations
  - Both Y datasets were explicitly changed to factors
  - The feature 'V8' was found to be completely missing in all X datasets. Since the feature was missing entirely without even a suggestion as to what type of data it was, no imputation scheme would be able to populate it, so the feature was removed entirely. This decreased the total count of missing values significantly in each X dataset.
- Insights
  - Since the response datasets were categorical, the proper method to use for making predications was classification.
  - No extreme observations were found

Step 2: Evaluating and handling missing data
- An attempt was made to visualize the data with a ggplot function, but the datasets were too large to visualize without
- K nearest neighbors was used to impute the missing data for all numerical features. K nearest neighbors was selected because it is relatively simple, quick, the specific method used automatically centers and scales the data which is useful, and more advanced techniques were probably unncessary given the very small number and randomness of the missing data in each dataset.
- Since KNN cannot be used for categorical variables, mode imputation was used for the 2 factors. Tt is also a quick and effective method for small amounts of missing data.
- The factors were then converted to dummy variables for modeling.

Step 3: Evaluating the presence of multicollinearity among the features
- Once the datasets had no missing values, it was possible to look at correlations between all the features for every observation. This revealed strong multicollinearity in the datasets, with some very high correlations up to 0.6.
- An attempt was made to model the data at this point with a simple logistic regression to look at variable inflation factors. The VIF values were also quite high, with the highest being 12. The model was not even able to converge because of the high degree of multicollinearity.
- Because a regular logistic regression was not even able to converge, some degree of feature selection would need to take place either before or during modeling. A logistic elastic net model with k-fold cross validation was chosen because it combines the properties of simultaneous feature selection and coefficient shrinkage.

Step 4: Modeling the data
- A k value of 10 was used for f-fold cross validation. 
- In order to fairly exhaustively find the best tuning parameters for the elastic net model, a tuning grid was used that contained 100 alpha values from 0.5 to 1 and 100 lambda values from 0.0001 to 0.5.
- The best tuning parameter values found in this analysis were:
  Training data:
    alpha = 0.5
    lambda = 0.4596
  Validation data:
    alpha = 0.5
    lambda = 0.5
- The data was modeled using glmnet, and predictions were made on the test data. The predictions probabilities were classified as either 'not fraud' or 'fraud' based on whether the probability was less than or greater than 0.5, respectively.
- The confusion matrices and model metrics for the training and validation models are below:
 
 Confusion Matrix and measurements for Training Model

Prediction  fraud not fraud
  fraud      3343         0
  not fraud     1      1656
           Reference

Accuracy	0.9998000			
Kappa	0.9995486			
AccuracyLower	0.9988862			
AccuracyUpper	0.9999949
Sensitivity	0.9997010			
Specificity	1.0000000			
Pos Pred Value	1.0000000			
Neg Pred Value	0.9993965			
Precision	1.0000000			
Recall	0.9997010			
F1	0.9998505			
Prevalence	0.6688000			
Detection Rate	0.6686000			
Detection Prevalence	0.6686000	


Confusion Matrix and measurements for Validation Model        

Prediction  fraud not fraud
  fraud       617         3
  not fraud     0       380

Accuracy	0.9970000		
Kappa	0.9936430	
AccuracyLower	0.9912580			
AccuracyUpper	0.9993809			
Sensitivity	1.0000000			
Specificity	0.9921671			
Pos Pred Value	0.9951613			
Neg Pred Value	1.0000000			
Precision	0.9951613			
Recall	1.0000000			
F1	0.9975748			
Prevalence	0.6170000			
Detection Rate	0.6170000			
Detection Prevalence	0.6200000	

- Since both models proved to be very successful at predicting the response for both datasets, no further analysis, like looking at the Receiver Operating Characteristic Curve or the area under that curve, was done.