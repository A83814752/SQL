% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\begin{longtable}[]{@{}l@{}}
\toprule
\endhead
title: ``Final Part 1, Predictions''\tabularnewline
output:\tabularnewline
pdf\_document\tabularnewline
documentclass: article\tabularnewline
classoption: a4paper\tabularnewline
\bottomrule
\end{longtable}

The analysis of the given data was divided into the following steps:

1 Examining the data structure and transforming the data 2 Evaluating
and handling missing data 3 Evaluating the presence of multicollinearity
among the features 4 Modeling the data

Step 1: Examining the data structure and transforming the data

\begin{itemize}
\tightlist
\item
  Dataset structure

  \begin{itemize}
  \tightlist
  \item
    The Xtrain dataset contained 5000 observations and 255 features, 2
    of which were factors, the rest numeric.
  \item
    The Xtrain dataset had 6287 missing values, but no more than 12 in
    any given feature, and the distribution appeared to be Missing at
    Random.
  \item
    The Xvalidate dataset contained 1000 observations and 255 features,
    2 of which were factors, the rest numeric.
  \item
    The Xvalidate dataset had 1251 missing values, but no more than 5 in
    any given feature, and the distribution appeared to be Missing at
    Random.
  \item
    The Xtest dataset contained 1000 observations and 255 features, 2 of
    which were factors, the rest numeric.
  \item
    The Xtest dataset had 1274 missing values, but no more than 4 in any
    given feature, and the distribution appeared to be Missing at
    Random.
  \item
    The Ytrain dataset contained 5000 values with 2 factors, `fraud' and
    `not fraud'
  \item
    The Yvalidate dataset contained 1000 values with 2 factors, `fraud'
    and `not fraud'
  \end{itemize}
\item
  Data transformations

  \begin{itemize}
  \tightlist
  \item
    Both Y datasets were explicitly changed to factors
  \item
    The feature `V8' was found to be completely missing in all X
    datasets. Since the feature was missing entirely without even a
    suggestion as to what type of data it was, no imputation scheme
    would be able to populate it, so the feature was removed entirely.
    This decreased the total count of missing values significantly in
    each X dataset.
  \end{itemize}
\item
  Insights

  \begin{itemize}
  \tightlist
  \item
    Since the response datasets were categorical, the proper method to
    use for making predications was classification.
  \item
    No extreme observations were found
  \end{itemize}
\end{itemize}

Step 2: Evaluating and handling missing data - An attempt was made to
visualize the data with a ggplot function, but the datasets were too
large to visualize without - K nearest neighbors was used to impute the
missing data for all numerical features. K nearest neighbors was
selected because it is relatively simple, quick, the specific method
used automatically centers and scales the data which is useful, and more
advanced techniques were probably unncessary given the very small number
and randomness of the missing data in each dataset. - Since KNN cannot
be used for categorical variables, mode imputation was used for the 2
factors. Tt is also a quick and effective method for small amounts of
missing data. - The factors were then converted to dummy variables for
modeling.

Step 3: Evaluating the presence of multicollinearity among the features
- Once the datasets had no missing values, it was possible to look at
correlations between all the features for every observation. This
revealed strong multicollinearity in the datasets, with some very high
correlations up to 0.6. - An attempt was made to model the data at this
point with a simple logistic regression to look at variable inflation
factors. The VIF values were also quite high, with the highest being 12.
The model was not even able to converge because of the high degree of
multicollinearity. - Because a regular logistic regression was not even
able to converge, some degree of feature selection would need to take
place either before or during modeling. A logistic elastic net model
with k-fold cross validation was chosen because it combines the
properties of simultaneous feature selection and coefficient shrinkage.

Step 4: Modeling the data - A k value of 10 was used for f-fold cross
validation. - In order to fairly exhaustively find the best tuning
parameters for the elastic net model, a tuning grid was used that
contained 100 alpha values from 0.5 to 1 and 100 lambda values from
0.0001 to 0.5. - The best tuning parameter values found in this analysis
were: Training data: alpha = 0.5 lambda = 0.4596 Validation data: alpha
= 0.5 lambda = 0.5 - The data was modeled using glmnet, and predictions
were made on the test data. The predictions probabilities were
classified as either `not fraud' or `fraud' based on whether the
probability was less than or greater than 0.5, respectively. - The
confusion matrices and model metrics for the training and validation
models are below:

Confusion Matrix and measurements for Training Model

Prediction fraud not fraud fraud 3343 0 not fraud 1 1656 Reference

Accuracy 0.9998000\\
Kappa 0.9995486\\
AccuracyLower 0.9988862\\
AccuracyUpper 0.9999949 Sensitivity 0.9997010\\
Specificity 1.0000000\\
Pos Pred Value 1.0000000\\
Neg Pred Value 0.9993965\\
Precision 1.0000000\\
Recall 0.9997010\\
F1 0.9998505\\
Prevalence 0.6688000\\
Detection Rate 0.6686000\\
Detection Prevalence 0.6686000

Confusion Matrix and measurements for Validation Model

Prediction fraud not fraud fraud 617 3 not fraud 0 380

Accuracy 0.9970000\\
Kappa 0.9936430\\
AccuracyLower 0.9912580\\
AccuracyUpper 0.9993809\\
Sensitivity 1.0000000\\
Specificity 0.9921671\\
Pos Pred Value 0.9951613\\
Neg Pred Value 1.0000000\\
Precision 0.9951613\\
Recall 1.0000000\\
F1 0.9975748\\
Prevalence 0.6170000\\
Detection Rate 0.6170000\\
Detection Prevalence 0.6200000

\begin{itemize}
\tightlist
\item
  Since both models proved to be very successful at predicting the
  response for both datasets, no further analysis, like looking at the
  Receiver Operating Characteristic Curve or the area under that curve,
  was done.
\end{itemize}

\end{document}
